{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04dc14b-0b5a-4db2-b7fd-5580ef9193fd",
   "metadata": {
    "id": "c04dc14b-0b5a-4db2-b7fd-5580ef9193fd"
   },
   "source": [
    "# Fine-Tuning BERT & GPT (HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "oGHeMXeuhKYw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "oGHeMXeuhKYw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "96c198f9-006f-4252-83e3-c11a28ea9ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 27 09:18:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   60C    P0             30W /   70W |     102MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749f022e-1908-48d6-89de-e28eee8b8b93",
   "metadata": {
    "collapsed": true,
    "id": "749f022e-1908-48d6-89de-e28eee8b8b93",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364cd3be-fcaf-489e-9e48-55700893b9b2",
   "metadata": {
    "id": "364cd3be-fcaf-489e-9e48-55700893b9b2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c432eb6-324b-4bbd-9caa-6bccc341225f",
   "metadata": {
    "id": "7c432eb6-324b-4bbd-9caa-6bccc341225f"
   },
   "source": [
    "### Fine-Tune BERT (Text Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c9ade9e-ef6a-4759-a2c9-8a5e82b11341",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2c9ade9e-ef6a-4759-a2c9-8a5e82b11341",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6ef69c0b-22e8-4f25-df1d-8561163f437f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We fine-tune BERT on the SST2 sentiment dataset.\n",
    "dataset = load_dataset(\"sst2\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "835e7847-2920-4fe3-b336-407de782aefe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "835e7847-2920-4fe3-b336-407de782aefe",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1500fbc5-d4e5-4cba-f3df-599b6404068c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5770049d-2e05-49e8-bd53-3c7c74b573ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8bb636bf179845faaf99330683559f0a",
      "70471ad61da9499fac4d688d695d74b6",
      "278536b8b0aa4b91bffcb4e6dbbc0050",
      "8d7c7e7dbef0476cb8196fffb75cfdb5",
      "95ae6cb9a4e646eda22b9797e8ca30c5",
      "dafe74d33cf74504a6c8484c2053ccb0",
      "260e0b85af694cb4ac372c5d9ded01f2",
      "503d8b2dfc774dcaaa12762c5b46ce63",
      "00b39afbe36e4c22917d331057e40410",
      "541b26163a63498a9f7e618eb74af8ec",
      "0a1e819d62e44971b3e5f520384b7d5b"
     ]
    },
    "collapsed": true,
    "id": "5770049d-2e05-49e8-bd53-3c7c74b573ec",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7b08bada-856c-4611-bfa0-0ae2fa2c641b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb636bf179845faaf99330683559f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f133ccd-cabf-4124-822f-b7df7eb943ba",
   "metadata": {
    "id": "4f133ccd-cabf-4124-822f-b7df7eb943ba"
   },
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wNiEqP1QlFTr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "wNiEqP1QlFTr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "56dd05a8-de32-406d-bf46-0902396b0b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataCollatorWithPadding in module transformers.data.data_collator:\n",
      "\n",
      "class DataCollatorWithPadding(builtins.object)\n",
      " |  DataCollatorWithPadding(tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = True, max_length: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, return_tensors: str = 'pt') -> None\n",
      " |\n",
      " |  Data collator that will dynamically pad the inputs received.\n",
      " |\n",
      " |  Args:\n",
      " |      tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
      " |          The tokenizer used for encoding the data.\n",
      " |      padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
      " |          Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
      " |          among:\n",
      " |\n",
      " |          - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |            sequence is provided).\n",
      " |          - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |            acceptable input length for the model if that argument is not provided.\n",
      " |          - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
      " |      max_length (`int`, *optional*):\n",
      " |          Maximum length of the returned list and optionally padding length (see above).\n",
      " |      pad_to_multiple_of (`int`, *optional*):\n",
      " |          If set will pad the sequence to a multiple of the provided value.\n",
      " |\n",
      " |          This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
      " |          7.0 (Volta).\n",
      " |      return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n",
      " |          The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __call__(self, features: list[dict[str, typing.Any]]) -> dict[str, typing.Any]\n",
      " |      Call self as a function.\n",
      " |\n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __init__(self, tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = True, max_length: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, return_tensors: str = 'pt') -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'max_length': typing.Optional[int], 'pad_to_multipl...\n",
      " |\n",
      " |  __dataclass_fields__ = {'max_length': Field(name='max_length',type=typ...\n",
      " |\n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __match_args__ = ('tokenizer', 'padding', 'max_length', 'pad_to_multip...\n",
      " |\n",
      " |  max_length = None\n",
      " |\n",
      " |  pad_to_multiple_of = None\n",
      " |\n",
      " |  padding = True\n",
      " |\n",
      " |  return_tensors = 'pt'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DataCollatorWithPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "oRUv-NSBleEX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRUv-NSBleEX",
    "outputId": "a50e7d15-9b45-4362-95c7-f06636070ff5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.array([1,10,2,4]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c657e4a4-b870-4a48-8540-479982502387",
   "metadata": {
    "id": "c657e4a4-b870-4a48-8540-479982502387"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f87ac78b-0adc-4469-903d-86d74f961513",
   "metadata": {
    "id": "f87ac78b-0adc-4469-903d-86d74f961513"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"bert-finetuned-sst2\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "YMBSjOwywgF3",
   "metadata": {
    "id": "YMBSjOwywgF3"
   },
   "outputs": [],
   "source": [
    "#!pip install peft bitsandbytes accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "SNKojcX1w3t1",
   "metadata": {
    "id": "SNKojcX1w3t1"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "xjb_gGdQycQO",
   "metadata": {
    "id": "xjb_gGdQycQO"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(r=16,lora_alpha=32, target_modules=[\"query\",\"value\"], lora_dropout=0.05, bias = \"none\", task_type='SEQ_CLS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XuYVQal6J6SO",
   "metadata": {
    "id": "XuYVQal6J6SO"
   },
   "outputs": [],
   "source": [
    "help(LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "n_xhuveqxw4f",
   "metadata": {
    "id": "n_xhuveqxw4f"
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model,peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7FNtJEfhx3IP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "7FNtJEfhx3IP",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f27672ab-5aee-4076-d77b-70634f4da093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PeftModel.print_trainable_parameters of PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BertForSequenceClassification(\n",
      "      (bert): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSdpaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.print_trainable_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0QHTCVHZ0tqX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QHTCVHZ0tqX",
    "outputId": "7d83b4cf-4c06-4bdb-bc7a-c7ea14a67dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 591,362 || all params: 110,075,140 || trainable%: 0.5372\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "980cdb61-71e8-486e-b40e-32e29d3eff2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "980cdb61-71e8-486e-b40e-32e29d3eff2b",
    "outputId": "3a8b0f0b-af63-4f35-f102-8bd7400e9da7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-600156926.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aBAu3uiTy7Ku",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "aBAu3uiTy7Ku",
    "outputId": "5b5a4142-83d6-4071-ada1-d61a587b2cb9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8420' max='8420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8420/8420 10:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.283976</td>\n",
       "      <td>0.925459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.276515</td>\n",
       "      <td>0.925459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8420, training_loss=0.06799452276524343, metrics={'train_runtime': 642.9222, 'train_samples_per_second': 209.509, 'train_steps_per_second': 13.096, 'total_flos': 2453736217768344.0, 'train_loss': 0.06799452276524343, 'epoch': 2.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f305701-9640-4066-a2a4-33c76af2b820",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "0f305701-9640-4066-a2a4-33c76af2b820",
    "outputId": "8331017b-77d7-44e9-d255-095c68dcd019"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmadhuri-madhuris\u001b[0m (\u001b[33mmadhuri-madhuris-abc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251127_092230-rq404dpb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/madhuri-madhuris-abc/huggingface/runs/rq404dpb' target=\"_blank\">usual-oath-1</a></strong> to <a href='https://wandb.ai/madhuri-madhuris-abc/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/madhuri-madhuris-abc/huggingface' target=\"_blank\">https://wandb.ai/madhuri-madhuris-abc/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/madhuri-madhuris-abc/huggingface/runs/rq404dpb' target=\"_blank\">https://wandb.ai/madhuri-madhuris-abc/huggingface/runs/rq404dpb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8420' max='8420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8420/8420 18:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.256713</td>\n",
       "      <td>0.916284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.297938</td>\n",
       "      <td>0.926606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8420, training_loss=0.16934690362200885, metrics={'train_runtime': 1306.7244, 'train_samples_per_second': 103.081, 'train_steps_per_second': 6.444, 'total_flos': 2436910441971660.0, 'train_loss': 0.16934690362200885, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d9dd8-dac0-4698-b5aa-00beeebcdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QAT + LoRA on BERT (4-bit During Train)\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# QAT Config (4-bit sim: model \"sees\" noise during train)\n",
    "qat_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Reload model with QAT (apply LoRA on top)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2, quantization_config=qat_config, device_map=\"auto\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)  # Your r=16/alpha=32\n",
    "\n",
    "# Train with QAT sim (1 epoch subset)\n",
    "qat_args = TrainingArguments(\n",
    "    output_dir=\"qat-lora-bert\", num_train_epochs=1, per_device_train_batch_size=8,  # Smaller batch for sim\n",
    "    learning_rate=2e-5, gradient_checkpointing=True  # Memory for QAT\n",
    ")\n",
    "qat_trainer = Trainer(model=model, args=qat_args, train_dataset=tokenized[\"train\"].select(range(2000)),\n",
    "                      eval_dataset=tokenized[\"validation\"], tokenizer=tokenizer, data_collator=collator,\n",
    "                      compute_metrics=compute_metrics)\n",
    "qat_trainer.train()\n",
    "\n",
    "# Post-QAT: Quantize & Eval (should hold acc better)\n",
    "qat_model = qat_trainer.model  # Already sim'd\n",
    "qat_acc = qat_trainer.evaluate()[\"eval_accuracy\"]\n",
    "print(f\"QAT Train Acc: {qat_acc:.3f} (Holds better post-quant)\")\n",
    "\n",
    "# Save\n",
    "qat_trainer.save_model(\"qat-lora-bert-sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "GTfTwkZ2s6Y2",
   "metadata": {
    "id": "GTfTwkZ2s6Y2"
   },
   "outputs": [],
   "source": [
    "#trainer.save_model(\"lora-bert-sst2\")  # Save LoRA-merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7f137-27e2-4bb1-97d3-eef877e25302",
   "metadata": {
    "id": "cae7f137-27e2-4bb1-97d3-eef877e25302"
   },
   "source": [
    "##### Evaluate & Test BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5459a977-df1c-4512-ba7e-dbe5c5b5de98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5459a977-df1c-4512-ba7e-dbe5c5b5de98",
    "outputId": "b70b15f1-cea6-484b-ce8d-e814e6421732"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.995940089225769}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "clf(\"do you expect me to say it was a good movie though it was so boring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03aec6e-97cb-4ec1-b4e7-57b2e1853c13",
   "metadata": {
    "id": "d03aec6e-97cb-4ec1-b4e7-57b2e1853c13"
   },
   "source": [
    "##### Fine-Tune GPT2 (Text Generation)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dafc5287-f62f-4fa7-abe7-17a9afd61eb0",
   "metadata": {
    "id": "dafc5287-f62f-4fa7-abe7-17a9afd61eb0"
   },
   "source": [
    "GPT2 fine-tuning requires a prompt → continuation style dataset.\n",
    "\n",
    "We will use the wikitext dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca5d580-89fd-4255-bd6b-9a8429806387",
   "metadata": {
    "id": "eca5d580-89fd-4255-bd6b-9a8429806387"
   },
   "outputs": [],
   "source": [
    "gpt_name = \"gpt2\"\n",
    "gpt_tok = AutoTokenizer.from_pretrained(gpt_name)\n",
    "gpt_tok.pad_token = gpt_tok.eos_token\n",
    "\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(gpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e235096-7c51-4d47-9ff3-6664ab30ef30",
   "metadata": {
    "id": "1e235096-7c51-4d47-9ff3-6664ab30ef30"
   },
   "outputs": [],
   "source": [
    "text_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42c140-2e71-4d3e-860e-50202afb9977",
   "metadata": {
    "id": "0c42c140-2e71-4d3e-860e-50202afb9977"
   },
   "outputs": [],
   "source": [
    "def tokenize_gpt(batch):\n",
    "    return gpt_tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_text = text_ds.map(tokenize_gpt, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4f161-3ebd-4e58-b7cf-2a72810b529f",
   "metadata": {
    "id": "61e4f161-3ebd-4e58-b7cf-2a72810b529f"
   },
   "outputs": [],
   "source": [
    "gpt_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a927ee2-3812-40b8-8637-59a4b6c2cf8a",
   "metadata": {
    "id": "0a927ee2-3812-40b8-8637-59a4b6c2cf8a"
   },
   "outputs": [],
   "source": [
    "gpt_trainer = Trainer(\n",
    "    model=gpt_model,\n",
    "    args=gpt_args,\n",
    "    train_dataset=tokenized_text[\"train\"],\n",
    "    eval_dataset=tokenized_text[\"validation\"],\n",
    "    tokenizer=gpt_tok,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8995f-8092-4ddf-b1a3-1aaa8f0945c5",
   "metadata": {
    "id": "00b8995f-8092-4ddf-b1a3-1aaa8f0945c5"
   },
   "outputs": [],
   "source": [
    "gpt_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b69a3d-2445-4728-b4ef-da63a38e545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_trainer.save_model(\"gpt2-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf2e95-bc2b-4da1-8c9e-399423a09097",
   "metadata": {
    "id": "0faf2e95-bc2b-4da1-8c9e-399423a09097"
   },
   "source": [
    "##### Test GPT2 Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57988ca-269e-4949-8037-288b584ce191",
   "metadata": {
    "id": "a57988ca-269e-4949-8037-288b584ce191"
   },
   "outputs": [],
   "source": [
    "# GPT2 Generation Test\n",
    "gen_pipe = pipeline(\"text-generation\", model=gpt_model, tokenizer=gpt_tok, max_length=60)\n",
    "gen_pipe(\"Deep learning is a revolutionary field because\")[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1HhSaQG50Zix",
   "metadata": {
    "id": "1HhSaQG50Zix"
   },
   "outputs": [],
   "source": [
    "# Setup (run once)\n",
    "# !pip install -q transformers datasets peft bitsandbytes accelerate evaluate wandb\n",
    "# Imports (add to your notebook if needed)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, pipeline, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import time\n",
    "import wandb\n",
    "wandb.init(project=\"llm-workshop\", name=\"ptq-bert-demo\")  # Log to your project\n",
    "\n",
    "# Load your fine-tuned BERT from notebook\n",
    "model_dir = \"bert-finetuned-sst2\"  # From your trainer.save_model()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "full_model = AutoModelForSequenceClassification.from_pretrained(model_dir, torch_dtype=torch.float16)\n",
    "\n",
    "# Calibration data (SST-2 val subset)\n",
    "dataset = load_dataset(\"sst2\", split=\"validation[:128]\")\n",
    "def tokenize(batch): return tokenizer(batch[\"sentence\"], truncation=True, padding=True)\n",
    "calib_data = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# PTQ: Load in 8-bit (calibrates automatically)\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "quant_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_dir, quantization_config=quant_config, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Eval accuracy (full vs PTQ)\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)\n",
    "\n",
    "args = TrainingArguments(output_dir=\"ptq-temp\", per_device_eval_batch_size=16, no_cuda=False)  # Eval only\n",
    "trainer_full = Trainer(model=full_model, args=args, eval_dataset=calib_data, tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
    "full_acc = trainer_full.evaluate()[\"eval_accuracy\"]\n",
    "\n",
    "trainer_ptq = Trainer(model=quant_model, args=args, eval_dataset=calib_data, tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
    "ptq_acc = trainer_ptq.evaluate()[\"eval_accuracy\"]\n",
    "\n",
    "print(f\"Full Acc: {full_acc:.3f} | PTQ 8-bit Acc: {ptq_acc:.3f} | Drop: {full_acc - ptq_acc:.3f}\")\n",
    "\n",
    "# Speed test (your pipeline style)\n",
    "full_clf = pipeline(\"sentiment-analysis\", model=full_model, tokenizer=tokenizer)\n",
    "quant_clf = pipeline(\"sentiment-analysis\", model=quant_model, tokenizer=tokenizer)\n",
    "\n",
    "test_text = \"This movie is surprisingly good!\"\n",
    "start = time.time(); full_out = full_clf(test_text); full_time = time.time() - start\n",
    "start = time.time(); ptq_out = quant_clf(test_text); ptq_time = time.time() - start\n",
    "\n",
    "print(f\"Full Time: {full_time:.4f}s | PTQ Time: {ptq_time:.4f}s | Speedup: {full_time / ptq_time:.1f}x\")\n",
    "print(\"Sample PTQ Output:\", ptq_out)\n",
    "\n",
    "# Log to wandb\n",
    "wandb.log({\"full_acc\": full_acc, \"ptq_acc\": ptq_acc, \"speedup\": full_time / ptq_time})\n",
    "wandb.finish()\n",
    "\n",
    "# Optional: Save PTQ model\n",
    "quant_model.save_pretrained(\"ptq-bert-sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b905496-29e9-442a-8e21-4efe38dd5f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AdaLoraConfig\n",
    "adalora_config = AdaLoraConfig(\n",
    "    init_r=64, target_modules=[\"q_proj\", \"v_proj\"], lora_alpha=64, lora_dropout=0.05, task_type=TaskType.CAUSAL_LM,\n",
    "    use_rslora=True\n",
    ")\n",
    "gpt_model = get_peft_model(gpt_model, adalora_config)  # Reload if needed: from_pretrained(\"gpt2-finetuned\")\n",
    "gpt_model.print_trainable_parameters()  # ~0.1% (pruned)\n",
    "\n",
    "gpt_args.num_train_epochs = 1  # Reuse args\n",
    "gpt_trainer = Trainer(model=gpt_model, args=gpt_args, train_dataset=tokenized_text[\"train\"].select(range(500)),\n",
    "                      eval_dataset=tokenized_text[\"validation\"].select(range(100)), tokenizer=gpt_tok)\n",
    "gpt_trainer.train()\n",
    "gpt_trainer.save_model(\"adalora-gpt2\")\n",
    "\n",
    "gen_pipe = pipeline(\"text-generation\", model=gpt_model, tokenizer=gpt_tok)\n",
    "print(gen_pipe(\"AI is\", max_new_tokens=20)[0][\"generated_text\"])  # More fluent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "778e1c1e-13af-4e3b-b69c-88c3234acc20",
   "metadata": {},
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Teacher: Your LoRA-BERT (load saved)\n",
    "teacher_dir = \"lora-bert-sst2\"  # From LoRA save\n",
    "teacher_tok = AutoTokenizer.from_pretrained(teacher_dir)\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained(teacher_dir, torch_dtype=torch.float16)\n",
    "teacher.eval()  # Frozen\n",
    "\n",
    "# Student: DistilBERT (smaller, init from base)\n",
    "student_name = \"distilbert-base-uncased\"\n",
    "student_tok = DistilBertTokenizer.from_pretrained(student_name)\n",
    "student = DistilBertForSequenceClassification.from_pretrained(student_name, num_labels=2, torch_dtype=torch.float16)\n",
    "\n",
    "# KD Loss (temperature softens, alpha balances)\n",
    "temperature = 2.0\n",
    "alpha = 0.5\n",
    "def kd_loss(student_logits, teacher_logits, labels):\n",
    "    soft_teacher = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    soft_student = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    kl_loss = F.kl_div(soft_student, soft_teacher, reduction=\"batchmean\") * (temperature ** 2)\n",
    "    ce_loss = F.cross_entropy(student_logits, labels)\n",
    "    return alpha * kl_loss + (1 - alpha) * ce_loss\n",
    "\n",
    "# Data (your SST-2 train subset for demo)\n",
    "train_ds = load_dataset(\"sst2\", split=\"train[:1000]\")  # Subset\n",
    "def tokenize(batch): return student_tok(batch[\"sentence\"], truncation=True, padding=True)\n",
    "train_tokenized = train_ds.map(tokenize, batched=True)\n",
    "\n",
    "# Train loop (simple, 1 epoch – use HF Trainer for prod)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=2e-5)\n",
    "student.train()\n",
    "for epoch in range(1):  # 1 epoch demo\n",
    "    for batch in torch.utils.data.DataLoader(train_tokenized, batch_size=16, collate_fn=lambda x: tokenizer.pad(x, return_tensors=\"pt\")):\n",
    "        inputs = {k: v.to('cuda') for k, v in batch.items() if k in ['input_ids', 'attention_mask']}\n",
    "        labels = batch['label'].to('cuda')\n",
    "        \n",
    "        # Teacher forward (no grad)\n",
    "        with torch.no_grad():\n",
    "            teacher_out = teacher(**inputs).logits\n",
    "        # Student forward\n",
    "        student_out = student(**inputs).logits\n",
    "        \n",
    "        loss = kd_loss(student_out, teacher_out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} KD Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Eval student vs teacher (your metrics)\n",
    "eval_ds = load_dataset(\"sst2\", split=\"validation\")\n",
    "eval_tokenized = eval_ds.map(tokenize, batched=True)\n",
    "trainer_student = Trainer(model=student, args=TrainingArguments(output_dir=\"kd-temp\", per_device_eval_batch_size=16),\n",
    "                          eval_dataset=eval_tokenized, tokenizer=student_tok, compute_metrics=compute_metrics)\n",
    "student_acc = trainer_student.evaluate()[\"eval_accuracy\"]\n",
    "print(f\"Teacher Acc: ~0.92 | Student KD Acc: {student_acc:.3f} (Retain: {student_acc / 0.92 * 100:.1f}%)\")\n",
    "\n",
    "# Save student\n",
    "student.save_pretrained(\"kd-distilbert-sst2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833718a-d514-4cdd-875c-a0fdd93e97d4",
   "metadata": {},
   "source": [
    "Paid A100 GPU Variant (Colab Pro+/AWS p4d – ~1 min, Batch 32):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042b78d-b50a-4fec-bb9a-b1ac0c279e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, but:\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=2e-5)\n",
    "# In loop:\n",
    "for batch in torch.utils.data.DataLoader(train_tokenized, batch_size=32, collate_fn=lambda x: tokenizer.pad(x, return_tensors=\"pt\")):  # 2x batch\n",
    "    # ... rest unchanged\n",
    "args = TrainingArguments(..., per_device_eval_batch_size=32, bf16=True)  # A100 fast BF16\n",
    "trainer_student = Trainer(model=student, args=args, ...)  # Eval on full val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7338840-aa2b-4bc7-a671-212d08b262c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00b39afbe36e4c22917d331057e40410": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0a1e819d62e44971b3e5f520384b7d5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "260e0b85af694cb4ac372c5d9ded01f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "278536b8b0aa4b91bffcb4e6dbbc0050": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_503d8b2dfc774dcaaa12762c5b46ce63",
      "max": 872,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_00b39afbe36e4c22917d331057e40410",
      "value": 872
     }
    },
    "503d8b2dfc774dcaaa12762c5b46ce63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "541b26163a63498a9f7e618eb74af8ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70471ad61da9499fac4d688d695d74b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dafe74d33cf74504a6c8484c2053ccb0",
      "placeholder": "​",
      "style": "IPY_MODEL_260e0b85af694cb4ac372c5d9ded01f2",
      "value": "Map: 100%"
     }
    },
    "8bb636bf179845faaf99330683559f0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_70471ad61da9499fac4d688d695d74b6",
       "IPY_MODEL_278536b8b0aa4b91bffcb4e6dbbc0050",
       "IPY_MODEL_8d7c7e7dbef0476cb8196fffb75cfdb5"
      ],
      "layout": "IPY_MODEL_95ae6cb9a4e646eda22b9797e8ca30c5"
     }
    },
    "8d7c7e7dbef0476cb8196fffb75cfdb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_541b26163a63498a9f7e618eb74af8ec",
      "placeholder": "​",
      "style": "IPY_MODEL_0a1e819d62e44971b3e5f520384b7d5b",
      "value": " 872/872 [00:00&lt;00:00, 4947.24 examples/s]"
     }
    },
    "95ae6cb9a4e646eda22b9797e8ca30c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dafe74d33cf74504a6c8484c2053ccb0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
